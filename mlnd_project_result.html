<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-06-30 Sat 23:25 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="tutysara" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbacd852">1. Introduction</a></li>
<li><a href="#org4beb9b0">2. Method</a></li>
<li><a href="#orgc40e138">3. Result</a></li>
<li><a href="#orgedb4e26">4. Webapp</a></li>
<li><a href="#orgfff020e">5. Conclusion</a></li>
<li><a href="#org23196d0">6. Future work</a></li>
<li><a href="#org06ebcf4">7. References</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgbacd852" class="outline-2">
<h2 id="orgbacd852"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
I tried to replicate the results in the paper, Cross-Media Learning for Image Sentiment Analysis in the Wild by Lucia Vadicamo et al.
Here we train an image classifier to classify images as having positive, negative or neutral sentiment.
A novel approach is used where no supervised labeling of data is necessary, instead the sentiment of the image is got from the text accacompanying the image in the tweet.
The dataset is available from their site <a href="http://www.t4sa.it/">http://www.t4sa.it/</a>.
The B-T4SA dataset is used which contains a balanced number of positive, neutral and negative images. 
The dataset is 63GB in size and contains 156,862 for each category resulting in a total of 470,586 images.
</p>
</div>
</div>
<div id="outline-container-org4beb9b0" class="outline-2">
<h2 id="org4beb9b0"><span class="section-number-2">2</span> Method</h2>
<div class="outline-text-2" id="text-2">
<p>
The first step is preprocessing the images so, that it can be read quickly of the disk.
Reading 63GB of image data and converting in to a tensor for an epoch is a time consuming process. 
I found that transforming the images into tensors and saving it to disk helps in avoiding the transformation cost across different runs.
The data is saved in  bcolz format, which is a disk backed datastructure similar to dataframe. It helped keeping the code to read data simple and offered fast read performance.
</p>

<p>
The method given in the paper is followed to replicate its results. The paper uses caffe, I did the experiment in Keras which was thought in the class.
The VGG19 model in Keras is not similar to the caffe implementation. It was altered by adding l2 regularizers to all conv layers to support l2<sub>weight</sub> decay similar to caffe.
</p>

<p>
Two version of the VGG19 model are finetuned. In one instance all the layers are freezed except for the bottleneck layers and is called VGG19-FT-F.
This is finetuned for 15 epochs using SGD with a lr rate of 1e-3. The learning rate is reduced by a factor of 10 once every 5 epochs to stop divergence of result.
A batch size of 64 is used while training, this is opposed to the batch size of 32 used in the paper. I found that they use batch accumulation of 2 to lower GPU footprint.
So, in effect it is equivalent to using a batch size of 64. My GPU could handle a batch size of 64 and I didn't had to make use of special techniques like batch accumulation to fit it in memory.
</p>

<p>
In the second instance all layers of the model are finetuned and this is called VGG19-FT-A. This is also trained for 15 epochs using SGD with a lr of 1e-3.
The batch size was however reduced to 48 to fit the model in memory.
</p>

<p>
Inaddition to the two instances of the VGG19 model, other types of models were also evaluated. I had experimented with MobilenetV2 and InceptionResnetV2 architechtures.
Two models are constructed for mobilenet namely MobilenetV2-FT-F and MobilenetV2-FT-A in which only the bottleneck layers and all layers are trained respectively.
Similarly two models are constructed based on InceptionResnetV2 namely InceptionResnetV2-FT-F and InceptionResnetV2-FT-A, where the bottleneck layers and all layers are trained respectively.
These 4 models are also finetuned similar to the VGG19 model for 15 epochs using SGD with a lr of 1e-3 and a batch size of 64 for *-FT-F models and 48 for *-FT-A models
</p>
</div>
</div>

<div id="outline-container-orgc40e138" class="outline-2">
<h2 id="orgc40e138"><span class="section-number-2">3</span> Result</h2>
<div class="outline-text-2" id="text-3">
<p>
I was able to replicate the result given in the paper.
I got an accuracy of  0.5042 with VGG19-FT-F and 0.5102 with VGG19-FT-A which is similar to the accuracy of 0.506 and 0.513 as reported in the paper
The more complex models didn't improved much on the accuracy. The MobilenetV2-FT-F got an accuracy of 0.4816 while the MobilenetV2-FT-A got an accuracy of 0.5108.
The inceptionresnet based model did slightly worse with InceptionResnetV2-FT-F getting 0.4727 and InceptionResnetV2-FT-A getting 0.5078.
This is summarized in the following table and graph
</p>

<p>
Table 1 : Accuracy of models on T4SA test set
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">0.5042</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">0.5102</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">0.4816</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">0.5108</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">0.4727</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">0.5078</td>
</tr>
</tbody>
</table>

<p>
Here is the graph of the result
</p>


<div class="figure">
<p><img src="./src/imgs/accuracy_t4sa_testset.png" alt="accuracy_t4sa_testset.png" style="float:right;" />
</p>
</div>

<p>
The accuracies of the models on the Twitter Training Dataset is also measured.
Here we can see that the more complex models like InceptionResnetV2-FT-A gives a better performance compared to less complex models like  VGG19-FT-A and MobilenetV2-FT-A.
We can also observe that the model fine tuned on all layers were able to catch the patterns in the data better compared to the models that are finetuned only on the fully connected layers
The results are summarized in the following table and graph
</p>

<p>
Table 2 : Accuracy of models on Twitter Testing dataset
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">0.740363</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">0.755102</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">0.722222</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">0.757370</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">0.714286</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">0.768707</td>
</tr>
</tbody>
</table>



<div class="figure">
<p><img src="./src/imgs/accuracy_twitter_training_dataset.png" alt="accuracy_twitter_training_dataset.png" style="float:right;" />
</p>
</div>

<p>
The training time and inference time are also important factors in determining the usefulness of a machine learning model in a constrained environment.
Most real world uses care about the training time and even more important is the inference time.
Training time is calculated on the GPU while the inference time is calculated on the CPU.
This is because most of the time training is done on a machine with a powerful GPU and the trained model is usually deployed on webservers which supports only CPUs.
The trained models are also deployed on less powerful devices like mobile phones so, the inference performance is calculated on CPU only system.
</p>

<p>
The models are trained for 15 epochs with EarlyStopping callback with a patience of 5.
So, if the validation loss doesn't improve for 5 consecutive epochs then the training is stopped even before the completion of 15 epochs.
The time is calculated as a ratio relative to the quickest model to train. This is to account for difference in the computing infrastructure and make meaningful comparision between models
We can see that the InceptionResnetV2 is the fastest to train. It converged quickly and got stopped early since there is not much improvement in the later epochs.
We can also observe that models where all layers are trained converge quickly than training only fully connected layers
</p>

<p>
Here is the table and graph of the result
</p>

<p>
Table 3 : Training time of models with T4SA training data.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Relative Training Time</th>
<th scope="col" class="org-right">Actual Training Time in hours</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">2.316</td>
<td class="org-right">178005</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">1.098</td>
<td class="org-right">84405</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">2.692</td>
<td class="org-right">206839</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">2.0</td>
<td class="org-right">153675</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">2.004</td>
<td class="org-right">153977</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">1.0</td>
<td class="org-right">76846</td>
</tr>
</tbody>
</table>


<div class="figure">
<p><img src="./src/imgs/training_time_comparision.png" alt="training_time_comparision.png" style="float:right;" />
</p>
</div>

<p>
The inference time is calculated by running the models on a subset of twitter training data.
Inference time is calculated on the CPU since the models will be usually deployed on a webserver which has only CPU.
</p>

<p>
Here is the table and graph of the result
</p>

<p>
Table 4: Average Inference time of models on Twitter Test data
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Average Inference Time (15 images) (S)</th>
<th scope="col" class="org-right">Standard Deviation (mS)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">10.4</td>
<td class="org-right">14.2</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">10.4</td>
<td class="org-right">45.8</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">1.78</td>
<td class="org-right">22.5</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">1.78</td>
<td class="org-right">12.7</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">5.25</td>
<td class="org-right">10.5</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">5.26</td>
<td class="org-right">30.0</td>
</tr>
</tbody>
</table>


<div class="figure">
<p><img src="./src/imgs/inference_time_comparision.png" alt="inference_time_comparision.png" style="float:right;" />
</p>
</div>

<p>
We see that MobilenetV2 has the quickest test time followed by InceptionResnetV2 and VGG19 models.
If are looking for a model that is quick with acceptable accuracy MobilenetV2 model is the way to go.
</p>

<p>
In addition to the training time, inference time and the accuracy of the models we should also consider the resource required to train the models.
Here the memory usage and CPU usage of the models are compared against one another. 
We use a relative comparision here since the actual numbers will depend on the actual hardware that is used and we are interested only the relative ease with which the models can be trained.
</p>

<p>
Here is the table and graph for CPU usage
</p>

<p>
Table 5: Average Relative CPU load while training on T4SA training data
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Average Relative Load</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">1.0</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">1.705</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">1.124</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">1.281</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">1.194</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">2.775</td>
</tr>
</tbody>
</table>


<div class="figure">
<p><img src="./src//imgs/training_cpu_usage_comparision.png" alt="training_cpu_usage_comparision.png" style="float:right;" />
</p>
</div>

<p>
We can see that InceptionResnetV2 models are high on CPU load while the MobilenetV2 has the least CPU load and the VGG19 models are inbetween.
</p>

<p>
Here is the table and graph for vsize usage
</p>

<p>
Table 6: Average Relative vsize while training on T4SA training data
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Average Relative Memory Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">1.354</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">1.364</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">1.0</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">1.301</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">1.04</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">1.369</td>
</tr>
</tbody>
</table>


<div class="figure">
<p><img src="./src/imgs/training_vsize_usage_comparision.png" alt="training_vsize_usage_comparision.png" style="float:right;" />
</p>
</div>


<p>
We can see that MobilenetV2 models has less memory demand followed by InceptionResnetV2, while the VGG19 models places a heavy demand on memory
</p>

<p>
Finally we also measure and compare the memory footprint of the trained and saved models.
Memory footprint plays an important role since the model has to be loaded into memory when deployed.
It determines the memory usage and subsequently the cost of running the application since many cloud providers charge for infrastructure based on the memory requirement.
It also determines the load time and scaling out time, a model with less memory footprint is quick to load and as a result easy to scale out.
Smaller models are also preferred on mobile devices because of limited resources and low bandwidth connection.
An app with a small apk size can be downloaded and used easily and can be updated frequently.
</p>

<p>
Here is the table and graph for memory footprint
</p>

<p>
Table 7: Memory footprint of models trained on T4SA training data
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">Size on disk in bytes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">VGG19-FT-F</td>
<td class="org-right">1036656072</td>
</tr>

<tr>
<td class="org-left">VGG19-FT-A</td>
<td class="org-right">1116766456</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-F</td>
<td class="org-right">15544744</td>
</tr>

<tr>
<td class="org-left">MobilenetV2-FT-A</td>
<td class="org-right">28407960</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-F</td>
<td class="org-right">219423224</td>
</tr>

<tr>
<td class="org-left">InceptionResnetV2-FT-A</td>
<td class="org-right">436709216</td>
</tr>
</tbody>
</table>


<div class="figure">
<p><img src="./src/imgs/trained_model_memory_footprint_comparision.png" alt="trained_model_memory_footprint_comparision.png" style="float:right;" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgedb4e26" class="outline-2">
<h2 id="orgedb4e26"><span class="section-number-2">4</span> Webapp</h2>
<div class="outline-text-2" id="text-4">
<p>
I created a webapp that can find the positive, neutral or negative sentiment of the uploaded image.
The webapp can be run on a commodity webserver with no GPU.
The software stack comprises of python with Flask framework for the backend and plain javascript along with Jquery is used for the frontend.
The MobilenetV2 based model MobilenetV2-FT-A is used to make the decision in the application, MobilenetV2 based models are used since they are
</p>
<ol class="org-ol">
<li>Light on resource usage (CPU and Memory)</li>
<li>Fast inference time on CPU</li>
<li>Small model footprint</li>
</ol>
</div>
</div>


<div id="outline-container-orgfff020e" class="outline-2">
<h2 id="orgfff020e"><span class="section-number-2">5</span> Conclusion</h2>
<div class="outline-text-2" id="text-5">
<p>
We used the labeled data from the B-T4SA dataset to train our models ( VGG19-FT-F,  VGG19-FT-A ).
I was able to replicate the results from the experiment using the architecture described in the paper, these are set as baseline models.
</p>

<p>
I fine tuned two models based on InceptionResnetV2 and two on MobilenetV2 architectures trained on imagenet data.
All these models are trained using the same training and validation data and tested against the test set from the downloaded B-T4SA dataset and TDD dataset found at DeepSent.
</p>

<p>
We then compared the new models performance to the baseline models and summarized the results in various such as
</p>
<ul class="org-ul">
<li>Accuracy</li>
<li>Training time</li>
<li>Inference time</li>
<li>CPU load</li>
<li>Memory load</li>
<li>Model Memory Footprint</li>
</ul>

<p>
We could see that the MobilenetV2 models achieved similar accuracy as the baseline models but has superior training and inference characteristics.
The  MobilenetV2-FT-A has a better accuracy compared to  MobilenetV2-FT-F, but it falls behind slightly on other performance characteristics compared to  MobilenetV2-FT-F model.
We have to make a tradeoff here and the  MobilenetV2-FT-A is selected for making the webapp, this is because the high accuracy and almost similar inference time as  MobilenetV2-FT-F model.
The  MobilenetV2-FT-F model beat MobilenetV2-FT-A model in certain characteristics like Training time, CPU load, Memory load and Model Memory Footprint.
But these are one time training cost and also the difference between  MobilenetV2-FT-A and MobilenetV2-FT-F are not drastic as compared to other models,
and they generally have the best performance characteristics of the three model types tested.
I selected the MobilenetV2 based model MobilenetV2-FT-A for creating the webapp based on the results and observation we have.
</p>

<p>
The InceptionResnetV2 based models were also better than the baseline models but the MobilenetV2 based models are still better.
</p>

<p>
I created a sample webapp based on MobilenetV2-FT-A model.
It is used to demonstrate deploying the model to a webserver as a microservice and I also created a basic frontend to interact with the service and label the uploaded image.
</p>
</div>
</div>


<div id="outline-container-org23196d0" class="outline-2">
<h2 id="org23196d0"><span class="section-number-2">6</span> Future work</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>Implement the solution with pytorch and try improving the accuracy, sample data from all three sections (train, test and valid) and test the upper limit on the accuracy that can be achieved on</li>
</ul>
<p>
this dataset
</p>
<ul class="org-ul">
<li>Implement circular learning rate and varying cycle from fast.ai library and try to train it faster.</li>
<li>Create a mobile app, deploy the model in mobile and check the framerate achieved, try to extend to find sentiment of scene from camera based on framerate reached.</li>
</ul>
</div>
</div>


<div id="outline-container-org06ebcf4" class="outline-2">
<h2 id="org06ebcf4"><span class="section-number-2">7</span> References</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>T4SA &#x2013; <a href="http://www.t4sa.it/">http://www.t4sa.it/</a></li>
<li>DeepSent &#x2013; <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html">https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html</a></li>
<li>Cross-Media Learning for Image Sentiment Analysis in the Wild
Lucia Vadicamo, Fabio Carrara, Andrea Cimino, Stefano Cresci, Felice Dell'Orletta, Fabrizio Falchi, Maurizio Tesconi <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w5/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.pdf">link</a></li>
<li>Quanzeng You, Jiebo Luo, Hailin Jin and Jianchao Yang, "Robust Image Sentiment Analysis using Progressively Trained and Domain Transferred Deep Networks", the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI), Austin, TX, January 25-30, 2015.<a href="https://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf">link</a></li>
<li>fast.ai &#x2013; <a href="https://github.com/fastai/fastai/">https://github.com/fastai/fastai/</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: tutysara</p>
<p class="date">Created: 2018-06-30 Sat 23:25</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
