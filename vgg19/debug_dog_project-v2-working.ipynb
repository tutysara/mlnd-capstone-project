{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tutysara/anaconda2/envs/dog-project/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bcolz\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from bcolzutils import *\n",
    "from util import *\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2 \n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"vgg19\"\n",
    "\n",
    "percent = 0.25\n",
    "#percent = 1\n",
    "epochs=15\n",
    "num_classes = 133\n",
    "batch_size = 64\n",
    "lr=1e-3\n",
    "momentum=0.9\n",
    "weight_decay = 1e-5\n",
    "test_prefix=\"\"\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\" divides the lr by 10 every 5 epochs\"\"\"\n",
    "    n = epoch // 5\n",
    "    return lr * (0.1 ** n)\n",
    "\n",
    "if percent < 1:\n",
    "    test_prefix = \"_test\"\n",
    "    \n",
    "test_result = f'bottleneck_features_{arch}_result{test_prefix}.npz'\n",
    "model_path = f'../saved_models/weights.best.topmodel.{arch}{test_prefix}.hdf5'\n",
    "loss_history_csv_name = f'train_top_model_{arch}_loss_history{test_prefix}.csv'\n",
    "\n",
    "d = datetime.datetime.today()\n",
    "\n",
    "logging.basicConfig(level='DEBUG',\n",
    "                    handlers=[\n",
    "                              logging.StreamHandler()])\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "basedir=\"/home/tutysara/src/myprojects/dog-project/dogImages\"\n",
    "\n",
    "train_idx_path = basedir+ \"/train_list.txt\"\n",
    "valid_idx_path = basedir+ \"/valid_list.txt\"\n",
    "test_idx_path = basedir+ \"/test_list.txt\"\n",
    "\n",
    "train_name = basedir + '/train'\n",
    "valid_name = basedir + '/valid'\n",
    "test_name = basedir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 532\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[0m\u001b[01;34mbottleneck_features_vgg19_test_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[01;34mbottleneck_features_vgg19_test_labels.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:12 \u001b[01;34mbottleneck_features_vgg19_test_y_pred.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:12 \u001b[01;34mbottleneck_features_vgg19_test_y_true.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[01;34mbottleneck_features_vgg19_train_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[01;34mbottleneck_features_vgg19_train_labels.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[01;34mbottleneck_features_vgg19_valid_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 20:00 \u001b[01;34mbottleneck_features_vgg19_valid_labels.bclz\u001b[0m/\n",
      "drwxr-xr-x 135 tutysara tutysara   4096 Mar 27  2017 \u001b[01;34mtest\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mtest_data_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mtest_data_labels.bclz\u001b[0m/\n",
      "-rw-rw-r--   1 tutysara tutysara  45864 Dec 24 15:27 test_list.txt\n",
      "drwxr-xr-x 135 tutysara tutysara   4096 Mar 27  2017 \u001b[01;34mtrain\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mtrain_data_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mtrain_data_labels.bclz\u001b[0m/\n",
      "-rw-rw-r--   1 tutysara tutysara 373348 Dec 24 15:25 train_list.txt\n",
      "drwxr-xr-x 135 tutysara tutysara   4096 Mar 27  2017 \u001b[01;34mvalid\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mvalid_data_data.bclz\u001b[0m/\n",
      "drwxrwxr-x   4 tutysara tutysara   4096 Mar  2 19:52 \u001b[01;34mvalid_data_labels.bclz\u001b[0m/\n",
      "-rw-rw-r--   1 tutysara tutysara  46664 Dec 24 15:26 valid_list.txt\n"
     ]
    }
   ],
   "source": [
    "%ls -l {basedir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate image loading\n",
    "# define function to load train, test, and validation datasets\n",
    "from sklearn.datasets import load_files \n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "valid_files, valid_targets = load_dataset(basedir + '/../' +'dogImages/valid')\n",
    "test_files, test_targets = load_dataset(basedir + '/../' +'dogImages/test')\n",
    "train_files, train_targets = load_dataset(basedir + '/../' +'dogImages/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices = random.sample(range(len(train_files)), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/082.Havanese/Havanese_05609.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/046.Cavalier_king_charles_spaniel/Cavalier_king_charles_spaniel_03298.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/056.Dachshund/Dachshund_03952.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/018.Beauceron/Beauceron_01324.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/109.Norwegian_elkhound/Norwegian_elkhound_07151.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/029.Border_collie/Border_collie_02005.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/064.English_toy_spaniel/English_toy_spaniel_04520.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/075.Glen_of_imaal_terrier/Glen_of_imaal_terrier_05141.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/123.Pomeranian/Pomeranian_07864.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/076.Golden_retriever/Golden_retriever_05186.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/036.Briard/Briard_02510.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/002.Afghan_hound/Afghan_hound_00092.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/048.Chihuahua/Chihuahua_03449.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/016.Beagle/Beagle_01133.jpg'\n",
      " '/home/tutysara/src/myprojects/dog-project/dogImages/../dogImages/train/011.Australian_cattle_dog/Australian_cattle_dog_00789.jpg']\n",
      "[ 81  45  55  17 108  28  63  74 122  75  35   1  47  15  10]\n"
     ]
    }
   ],
   "source": [
    "print(train_files[indices])\n",
    "print(np.argmax(train_targets[indices], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_size = int(valid_files.shape[0]*percent)\n",
    "test_data_size = int(test_files.shape[0]*percent)\n",
    "train_data_size = int(train_files.shape[0]*percent)\n",
    "\n",
    "if percent < 1:\n",
    "    valid_files = valid_files[:valid_data_size]\n",
    "    valid_targets = valid_targets[:valid_data_size]\n",
    "    \n",
    "    test_files = test_files[:test_data_size]\n",
    "    test_targets = test_targets[:test_data_size]\n",
    "    \n",
    "    train_files = train_files[:train_data_size]\n",
    "    train_targets = train_targets[:train_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208,) (208, 133)\n",
      "(209,) (209, 133)\n",
      "(1670,) (1670, 133)\n"
     ]
    }
   ],
   "source": [
    "print(valid_files.shape, valid_targets.shape)\n",
    "print(test_files.shape, test_targets.shape)\n",
    "print(train_files.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:02<00:00, 91.04it/s] \n",
      "100%|██████████| 209/209 [00:01<00:00, 113.14it/s]\n",
      "100%|██████████| 1670/1670 [00:17<00:00, 94.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# convert and load images\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "# load and preprocess data\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "# pre-process the data for Keras\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "mobilenet_feature_ext = MobileNet(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "bottleneck_features_validation = mobilenet_feature_ext.predict(preprocess_input(valid_tensors))\n",
    "bottleneck_features_test = mobilenet_feature_ext.predict(preprocess_input(test_tensors))\n",
    "bottleneck_features_train = mobilenet_feature_ext.predict(preprocess_input(train_tensors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valid_data = bottleneck_features_validation\n",
    "test_data = bottleneck_features_test\n",
    "train_data = bottleneck_features_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(valid_data.shape, valid_targets.shape)\n",
    "print(test_data.shape, test_targets.shape)\n",
    "print(train_data.shape, train_targets.shape)\n",
    "print(train_tensors.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(GlobalAveragePooling2D(input_shape=train_data.shape[1:]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.resnet50.hdf5', verbose=1, save_best_only=True)\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.mobilenet.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=25,\n",
    "          batch_size=64,\n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = vgg19_preprocess_input(valid_tensors, mode='caffe')\n",
    "test_data = vgg19_preprocess_input(test_tensors, mode='caffe')\n",
    "train_data = vgg19_preprocess_input(train_tensors, mode='caffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tutysara/anaconda2/envs/dog-project/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"my...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Generate a model with all layers (with top)\n",
    "vgg19 = VGG19(weights='imagenet', include_top=True)\n",
    "\n",
    "#Add a layer where input is the output of the  second last layer \n",
    "x = Dense(num_classes, activation='softmax', name='my_predictions')(vgg19.layers[-2].output)\n",
    "\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Then create the corresponding model \n",
    "my_model = Model(input=vgg19.input, output=x)\n",
    "my_model.layers[-3].trainable = True\n",
    "my_model.layers[-2].trainable = True\n",
    "my_model.layers[-1].trainable = True\n",
    "#my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 False\n",
      "block1_conv1 False\n",
      "block1_conv2 False\n",
      "block1_pool False\n",
      "block2_conv1 False\n",
      "block2_conv2 False\n",
      "block2_pool False\n",
      "block3_conv1 False\n",
      "block3_conv2 False\n",
      "block3_conv3 False\n",
      "block3_conv4 False\n",
      "block3_pool False\n",
      "block4_conv1 False\n",
      "block4_conv2 False\n",
      "block4_conv3 False\n",
      "block4_conv4 False\n",
      "block4_pool False\n",
      "block5_conv1 False\n",
      "block5_conv2 False\n",
      "block5_conv3 False\n",
      "block5_conv4 False\n",
      "block5_pool False\n",
      "flatten False\n",
      "fc1 True\n",
      "fc2 True\n",
      "my_predictions True\n"
     ]
    }
   ],
   "source": [
    "for layer in my_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1670 samples, validate on 208 samples\n",
      "Epoch 1/15\n",
      "1670/1670 [==============================] - 18s 11ms/step - loss: 2.7432 - acc: 0.4174 - val_loss: 1.6216 - val_acc: 0.6106\n",
      "Epoch 2/15\n",
      "1670/1670 [==============================] - 15s 9ms/step - loss: 0.3939 - acc: 0.8982 - val_loss: 1.2135 - val_acc: 0.6731\n",
      "Epoch 3/15\n",
      "1670/1670 [==============================] - 15s 9ms/step - loss: 0.1427 - acc: 0.9743 - val_loss: 0.9019 - val_acc: 0.7596\n",
      "Epoch 4/15\n",
      "1670/1670 [==============================] - 15s 9ms/step - loss: 0.0309 - acc: 0.9934 - val_loss: 0.9180 - val_acc: 0.7356\n",
      "Epoch 5/15\n",
      "1670/1670 [==============================] - 15s 9ms/step - loss: 0.0053 - acc: 0.9994 - val_loss: 0.8482 - val_acc: 0.7308\n",
      "Epoch 6/15\n",
      "1670/1670 [==============================] - 15s 9ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.8452 - val_acc: 0.7404\n",
      "Epoch 7/15\n",
      " 512/1670 [========>.....................] - ETA: 9s - loss: 0.0017 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7573f71d424d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           callbacks=[early_stopping, lrscheduler])\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dog-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "my_model.fit( train_data, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_feature_ext = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "bottleneck_features_validation = vgg19_feature_ext.predict(vgg19_preprocess_input(valid_tensors))\n",
    "bottleneck_features_test = vgg19_feature_ext.predict(vgg19_preprocess_input(test_tensors))\n",
    "bottleneck_features_train = vgg19_feature_ext.predict(vgg19_preprocess_input(train_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = num_classes\n",
    "\n",
    "  \n",
    "top_model = Sequential()\n",
    "top_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))\n",
    "#top_model.add(Flatten(input_shape=(7, 7, 512)))\n",
    "top_model.add(Dense(4096, activation='relu', name='fc1'))\n",
    "top_model.add(Dropout(0.5, name='fc1-dropout'))\n",
    "top_model.add(Dense(4096, activation='relu', name='fc2'))\n",
    "top_model.add(Dropout(0.5, name='fc2-dropout'))\n",
    "top_model.add(Dense(classes, activation='softmax', name='predictions'))\n",
    "\n",
    "#top_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "top_model.fit(bottleneck_features_train, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(bottleneck_features_validation, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 read data and convert to tensor\n",
    "col_names = [\"X\", \"y\"]\n",
    "train_data_df = pd.read_csv(train_idx_path, sep=\" \", header=None, names=col_names)\n",
    "valid_data_df = pd.read_csv(valid_idx_path, sep=\" \", header=None, names=col_names)\n",
    "test_data_df = pd.read_csv(test_idx_path, sep=\" \", header=None, names=col_names)\n",
    "\n",
    "train_data_df = train_data_df[:int(train_data_df.shape[0]*percent)]\n",
    "valid_data_df = valid_data_df[:int(valid_data_df.shape[0]*percent)]\n",
    "test_data_df = test_data_df[:int(test_data_df.shape[0]*percent)]\n",
    "\n",
    "train_data_df.y = train_data_df.y-1\n",
    "valid_data_df.y = valid_data_df.y-1\n",
    "test_data_df.y = test_data_df.y-1\n",
    "\n",
    "valid_files = valid_data_df.X.apply(lambda x: basedir+\"/\"+x)\n",
    "test_files = test_data_df.X.apply(lambda x: basedir+\"/\"+x)\n",
    "train_files = train_data_df.X.apply(lambda x: basedir+\"/\"+x)\n",
    "\n",
    "valid_labels = np_utils.to_categorical(valid_data_df.y, num_classes) \n",
    "test_labels = np_utils.to_categorical(test_data_df.y, num_classes)\n",
    "train_labels = np_utils.to_categorical(train_data_df.y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_files[50:75].values)\n",
    "print(train_data_df[50:75].y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_files.shape, valid_labels.shape)\n",
    "print(test_files.shape, test_labels.shape)\n",
    "print(train_files.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "my_model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_labels),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 read bcolz data\n",
    "valid_data = bcolz.carray(rootdir= valid_name+'_data.bclz', mode='r')\n",
    "test_data = bcolz.carray(rootdir= test_name + '_data.bclz', mode='r')\n",
    "train_data = bcolz.carray(rootdir= train_name+ '_data.bclz', mode='r')\n",
    "\n",
    "\n",
    "valid_labels = bcolz.carray(rootdir= valid_name+'_labels.bclz', mode='r')\n",
    "test_labels = bcolz.carray(rootdir= test_name + '_labels.bclz', mode='r')\n",
    "train_labels = bcolz.carray(rootdir= train_name+ '_labels.bclz', mode='r')\n",
    "\n",
    "print(valid_data.shape, valid_labels.shape)\n",
    "print(test_data.shape, test_labels.shape) \n",
    "print(train_data.shape, train_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model with all layers (with top)\n",
    "vgg19 = VGG19(weights='imagenet', include_top=True)\n",
    "\n",
    "#Add a layer where input is the output of the  second last layer \n",
    "x = Dense(num_classes, activation='softmax', name='my_predictions')(vgg19.layers[-2].output)\n",
    "\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Then create the corresponding model \n",
    "my_model = Model(input=vgg19.input, output=x)\n",
    "my_model.layers[-1].trainable = True\n",
    "my_model.layers[-2].trainable = True\n",
    "my_model.layers[-3].trainable = True\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in my_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_data.shape, valid_labels.shape)\n",
    "print(test_data.shape, test_labels.shape) \n",
    "print(train_data.shape, train_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen =bcolz_data_generator(train_data, train_labels, batch_size=batch_size, preprocess=vgg19_preprocess_input)\n",
    "valid_gen =bcolz_data_generator(valid_data, valid_labels, batch_size=batch_size, preprocess=vgg19_preprocess_input)\n",
    "test_gen =bcolz_data_generator(test_data, test_labels, batch_size=batch_size, preprocess=vgg19_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_preprocess_input(tX, mode='caffe').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "my_model.fit_generator(train_gen,\n",
    "          steps_per_epoch= (1 + int(train_data.shape[0] // batch_size)),\n",
    "          epochs=epochs,\n",
    "          validation_data=valid_gen,\n",
    "          validation_steps= (1 + int(valid_data.shape[0] // batch_size)),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data2 = vgg19_preprocess_input(valid_data, mode='caffe')\n",
    "test_data2 = vgg19_preprocess_input(test_data, mode='caffe')\n",
    "train_data2 = vgg19_preprocess_input(train_data, mode='caffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "my_model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_labels),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
