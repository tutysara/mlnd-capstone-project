{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tutysara/anaconda2/envs/dog-project/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bcolz\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from bcolzutils import *\n",
    "from util import *\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2 \n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:fit and save top mode using bottleneck features\n",
      "DEBUG:__main__:using top_model_weight_path../saved_models/weights.best.topmodel.vgg19.hdf5\n",
      "DEBUG:__main__:using test_resultbottleneck_features_vgg19_result.npz\n",
      "DEBUG:__main__:using loss_history_csv_nametrain_top_model_vgg19_loss_history.csv\n"
     ]
    }
   ],
   "source": [
    "arch = \"vgg19\"\n",
    "basedir=\"/home/tutysara/src/myprojects/dog-project/dogImages\"\n",
    "\n",
    "bnf_valid_name = basedir + f'/bottleneck_features_{arch}_valid'\n",
    "bnf_test_name = basedir + f'/bottleneck_features_{arch}_test' \n",
    "bnf_train_name = basedir + f'/bottleneck_features_{arch}_train'\n",
    "\n",
    "percent = 0.005\n",
    "percent = 1\n",
    "epochs=15\n",
    "num_classes = 133\n",
    "batch_size = 64\n",
    "lr=1e-3\n",
    "momentum=0.9\n",
    "weight_decay = 1e-5\n",
    "test_prefix=\"\"\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\" divides the lr by 10 every 5 epochs\"\"\"\n",
    "    n = epoch // 5\n",
    "    return lr * (0.1 ** n)\n",
    "\n",
    "if percent < 1:\n",
    "    test_prefix = \"_test\"\n",
    "    \n",
    "test_result = f'bottleneck_features_{arch}_result{test_prefix}.npz'\n",
    "model_path = f'../saved_models/weights.best.topmodel.{arch}{test_prefix}.hdf5'\n",
    "loss_history_csv_name = f'train_top_model_{arch}_loss_history{test_prefix}.csv'\n",
    "\n",
    "d = datetime.datetime.today()\n",
    "log_filename=f\"train_topmodel_bottleneck_{arch}_{d.year}-{d.month}-{d.day}-{d.hour}.{d.minute}.{d.second}{test_prefix}.log\"\n",
    "\n",
    "logging.basicConfig(level='DEBUG',\n",
    "                    handlers=[logging.FileHandler(log_filename),\n",
    "                              logging.StreamHandler()])\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.debug(\"fit and save top mode using bottleneck features\")\n",
    "log.debug(\"using top_model_weight_path\" + model_path)\n",
    "log.debug(\"using test_result\" + test_result)\n",
    "log.debug(\"using loss_history_csv_name\" + loss_history_csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "from sklearn.datasets import load_files \n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(basedir + '/../' +'dogImages/train')\n",
    "valid_files, valid_targets = load_dataset(basedir + '/../' +'dogImages/valid')\n",
    "test_files, test_targets = load_dataset(basedir + '/../' +'dogImages/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [01:09<00:00, 95.48it/s] \n",
      "100%|██████████| 835/835 [00:07<00:00, 106.47it/s]\n",
      "100%|██████████| 836/836 [00:07<00:00, 107.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert and load images\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "# load and preprocess data\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "               \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tensors.shape)\n",
    "print(valid_tensors.shape) \n",
    "print(test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "bnf_train_name = 'bottleneck_features_vgg19_train.npy'\n",
    "bnf_test_name = 'bottleneck_features_vgg19_test.npy' \n",
    "bnf_valid_name = 'bottleneck_features_vgg19_valid.npy'\n",
    "\n",
    "vgg19_feature_ext = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "bottleneck_features_train = vgg19_feature_ext.predict(preprocess_input(train_tensors))\n",
    "np.save(open(bnf_train_name, 'wb'),bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_validation = vgg19_feature_ext.predict(preprocess_input(valid_tensors))\n",
    "np.save(open(bnf_test_name, 'wb'),bottleneck_features_validation)\n",
    "\n",
    "bottleneck_features_test = vgg19_feature_ext.predict(preprocess_input(test_tensors))\n",
    "np.save(open(bnf_valid_name, 'wb'),bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load convered data back\n",
    "bnf_train_data = np.load(open(bnf_train_name, 'rb'))\n",
    "bnf_validation_data = np.load(open(bnf_test_name, 'rb'))\n",
    "bnf_test_data = np.load(open(bnf_valid_name, 'rb'))\n",
    "print(bnf_train_data.shape)\n",
    "print(bnf_validation_data.shape)\n",
    "print(bnf_test_data.shape)\n",
    "## go to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top model\n",
    "classes = num_classes\n",
    "\n",
    "  \n",
    "top_model = Sequential()\n",
    "top_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))\n",
    "#top_model.add(Flatten(input_shape=(7, 7, 512)))\n",
    "top_model.add(Dense(4096, activation='relu', name='fc1'))\n",
    "top_model.add(Dropout(0.5, name='fc1-dropout'))\n",
    "top_model.add(Dense(4096, activation='relu', name='fc2'))\n",
    "top_model.add(Dropout(0.5, name='fc2-dropout'))\n",
    "top_model.add(Dense(classes, activation='softmax', name='predictions'))\n",
    "\n",
    "#top_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "           \n",
    "\"\"\"             \n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=(7, 7, 512)))\n",
    "#top_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "top_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\"\"\" \n",
    "top_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "top_model.fit(bnf_train_data, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(bnf_validation_data, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load(basedir + '/../' + 'bottleneck_features/DogVGG19Data.npz')\n",
    "train_VGG19 = bottleneck_features['train']\n",
    "valid_VGG19 = bottleneck_features['valid']\n",
    "test_VGG19 = bottleneck_features['test']\n",
    "\n",
    "print(train_VGG19.shape, train_targets.shape)\n",
    "print(valid_VGG19.shape, valid_targets.shape)\n",
    "print(test_VGG19.shape, test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_model.fit(train_VGG19, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basedir=\"/media/hdd/datastore/t4sa\"\n",
    "batch_size = 128\n",
    "bnf_valid_name = basedir + f'/bottleneck_features_{arch}_valid'\n",
    "bnf_test_name = basedir + f'/bottleneck_features_{arch}_test' \n",
    "bnf_train_name = basedir + f'/bottleneck_features_{arch}_train'\n",
    "\n",
    "## Read it back from disk and check size\n",
    "bnf_valid_data = bcolz.carray(rootdir=f'{bnf_valid_name}_data.bclz', mode='r')\n",
    "bnf_test_data = bcolz.carray(rootdir=f'{bnf_test_name}_data.bclz', mode='r')\n",
    "bnf_train_data = bcolz.carray(rootdir=f'{bnf_train_name}_data.bclz', mode='r')\n",
    "\n",
    "bnf_valid_labels = bcolz.carray(rootdir=f'{bnf_valid_name}_labels.bclz', mode='r')\n",
    "bnf_test_labels = bcolz.carray(rootdir=f'{bnf_test_name}_labels.bclz', mode='r')\n",
    "bnf_train_labels = bcolz.carray(rootdir=f'{bnf_train_name}_labels.bclz', mode='r')\n",
    "\n",
    "log.debug(bnf_valid_data.shape)\n",
    "log.debug(bnf_test_data.shape)\n",
    "log.debug(bnf_train_data.shape)\n",
    "\n",
    "log.debug(bnf_valid_labels.shape)\n",
    "log.debug(bnf_test_labels.shape)\n",
    "log.debug(bnf_train_labels.shape)\n",
    "\n",
    "bnf_valid_data_size = int(bnf_valid_data.shape[0]*percent)\n",
    "bnf_test_data_size = int(bnf_test_data.shape[0]*percent)\n",
    "bnf_train_data_size = int(bnf_train_data.shape[0]*percent)\n",
    "\n",
    "if percent < 1:\n",
    "    bnf_valid_data = bnf_valid_data[:bnf_valid_data_size]\n",
    "    bnf_valid_labels = bnf_valid_labels[:bnf_valid_data_size]\n",
    "    \n",
    "    bnf_test_data = bnf_test_data[:bnf_test_data_size]\n",
    "    bnf_test_labels = bnf_test_labels[:bnf_test_data_size]\n",
    "    \n",
    "    bnf_train_data = bnf_train_data[:bnf_train_data_size]\n",
    "    bnf_train_labels = bnf_train_labels[:bnf_train_data_size]\n",
    "\n",
    "log.debug(\"loading percentage of original data from disk\")\n",
    "log.debug(bnf_valid_data.shape)\n",
    "log.debug(bnf_test_data.shape)\n",
    "log.debug(bnf_train_data.shape)\n",
    "\n",
    "log.debug(bnf_valid_labels.shape)\n",
    "log.debug(bnf_test_labels.shape)\n",
    "log.debug(bnf_train_labels.shape)\n",
    "\n",
    "bnf_train_gen =bcolz_data_generator(bnf_train_data, bnf_train_labels, batch_size=batch_size)\n",
    "bnf_valid_gen =bcolz_data_generator(bnf_valid_data, bnf_valid_labels, batch_size=batch_size)\n",
    "bnf_test_gen =bcolz_data_generator(bnf_test_data, bnf_test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "top_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_model.fit_generator(bnf_train_gen,\n",
    "          steps_per_epoch= (1 + int(bnf_train_data_size // batch_size)),\n",
    "          epochs=epochs,\n",
    "          validation_data=bnf_valid_gen,\n",
    "          validation_steps= (1 + int(bnf_valid_data_size // batch_size)),\n",
    "          callbacks=[early_stopping, lrscheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "my_predictions (Dense)       (None, 133)               544901    \n",
      "=================================================================\n",
      "Total params: 140,115,141\n",
      "Trainable params: 120,090,757\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tutysara/anaconda2/envs/dog-project/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"my...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Generate a model with all layers (with top)\n",
    "vgg19 = VGG19(weights='imagenet', include_top=True)\n",
    "\n",
    "#Add a layer where input is the output of the  second last layer \n",
    "x = Dense(num_classes, activation='softmax', name='my_predictions')(vgg19.layers[-2].output)\n",
    "\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Then create the corresponding model \n",
    "my_model = Model(input=vgg19.input, output=x)\n",
    "my_model.layers[-1].trainable = True\n",
    "my_model.layers[-2].trainable = True\n",
    "my_model.layers[-3].trainable = True\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 False\n",
      "block1_conv1 False\n",
      "block1_conv2 False\n",
      "block1_pool False\n",
      "block2_conv1 False\n",
      "block2_conv2 False\n",
      "block2_pool False\n",
      "block3_conv1 False\n",
      "block3_conv2 False\n",
      "block3_conv3 False\n",
      "block3_conv4 False\n",
      "block3_pool False\n",
      "block4_conv1 False\n",
      "block4_conv2 False\n",
      "block4_conv3 False\n",
      "block4_conv4 False\n",
      "block4_pool False\n",
      "block5_conv1 False\n",
      "block5_conv2 False\n",
      "block5_conv3 False\n",
      "block5_conv4 False\n",
      "block5_pool False\n",
      "flatten False\n",
      "fc1 True\n",
      "fc2 True\n",
      "my_predictions True\n"
     ]
    }
   ],
   "source": [
    "for layer in my_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = vgg19_preprocess_input(valid_tensors, mode='caffe')\n",
    "test_data = vgg19_preprocess_input(test_tensors, mode='caffe')\n",
    "train_data = vgg19_preprocess_input(train_tensors, mode='caffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "my_model.fit(train_data, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "my_model.fit(train_data, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = vgg19_preprocess_input(valid_tensors, mode='tf')\n",
    "test_data = vgg19_preprocess_input(test_tensors, mode='tf')\n",
    "train_data = vgg19_preprocess_input(train_tensors, mode='tf')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "my_model.fit(train_data, train_targets,\n",
    "          epochs=epochs,\n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = vgg19_preprocess_input(valid_tensors, mode='caffe')\n",
    "test_data = vgg19_preprocess_input(test_tensors, mode='caffe')\n",
    "train_data = vgg19_preprocess_input(train_tensors, mode='caffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "105/105 [==============================] - 61s 579ms/step - loss: 1.5842 - acc: 0.6283 - val_loss: 0.8993 - val_acc: 0.7317\n",
      "Epoch 2/15\n",
      "105/105 [==============================] - 56s 532ms/step - loss: 0.2134 - acc: 0.9432 - val_loss: 0.8378 - val_acc: 0.7760\n",
      "Epoch 3/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0632 - acc: 0.9856 - val_loss: 0.7427 - val_acc: 0.7832\n",
      "Epoch 4/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0272 - acc: 0.9958 - val_loss: 0.7280 - val_acc: 0.7916\n",
      "Epoch 5/15\n",
      "105/105 [==============================] - 56s 533ms/step - loss: 0.0168 - acc: 0.9975 - val_loss: 0.7168 - val_acc: 0.7928\n",
      "Epoch 6/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0114 - acc: 0.9981 - val_loss: 0.6984 - val_acc: 0.7940\n",
      "Epoch 7/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.6966 - val_acc: 0.7928\n",
      "Epoch 8/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.6957 - val_acc: 0.7916\n",
      "Epoch 9/15\n",
      "105/105 [==============================] - 56s 533ms/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.6955 - val_acc: 0.7916\n",
      "Epoch 10/15\n",
      "105/105 [==============================] - 56s 535ms/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.6956 - val_acc: 0.7916\n",
      "Epoch 11/15\n",
      "105/105 [==============================] - 56s 535ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.6956 - val_acc: 0.7916\n",
      "Epoch 12/15\n",
      "105/105 [==============================] - 56s 533ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.6957 - val_acc: 0.7916\n",
      "Epoch 13/15\n",
      "105/105 [==============================] - 56s 534ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.6957 - val_acc: 0.7916\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34ce3dfd68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen =bcolz_data_generator(train_data, train_targets, batch_size=batch_size)\n",
    "valid_gen =bcolz_data_generator(valid_data, valid_targets, batch_size=batch_size)\n",
    "test_gen =bcolz_data_generator(test_data, test_targets, batch_size=batch_size)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "csv_logger = CSVLogger(loss_history_csv_name, append=True, separator=',')\n",
    "lrscheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "my_model.fit_generator(train_gen,\n",
    "          steps_per_epoch= (1 + int(train_data.shape[0] // batch_size)),\n",
    "          epochs=epochs,\n",
    "          validation_data=valid_gen,\n",
    "          validation_steps= (1 + int(valid_data.shape[0] // batch_size)),\n",
    "          callbacks=[early_stopping, lrscheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
