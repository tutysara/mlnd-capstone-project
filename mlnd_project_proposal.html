<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-12-24 Sun 04:49 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Saravanan Baskaran" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf343243">1. Machine Learning Engineer Nanodegree</a>
<ul>
<li><a href="#orgfa6a9b6">1.1. Capstone Proposal</a></li>
<li><a href="#org40d4522">1.2. Proposal</a>
<ul>
<li><a href="#org4f6144f">1.2.1. Domain Background</a></li>
<li><a href="#org86f62b3">1.2.2. Problem Statement</a></li>
<li><a href="#orgfb687ee">1.2.3. Datasets and Inputs</a></li>
<li><a href="#orgb5ad733">1.2.4. Solution Statement</a></li>
<li><a href="#org7812995">1.2.5. Benchmark Model</a></li>
<li><a href="#orgc3a2384">1.2.6. Evaluation Metrics</a></li>
<li><a href="#org28524b2">1.2.7. Project Design</a></li>
<li><a href="#orgdb32f65">1.2.8. Future Work</a></li>
<li><a href="#org4a2a28e">1.2.9. Reference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgf343243" class="outline-2">
<h2 id="orgf343243"><span class="section-number-2">1</span> Machine Learning Engineer Nanodegree</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgfa6a9b6" class="outline-3">
<h3 id="orgfa6a9b6"><span class="section-number-3">1.1</span> Capstone Proposal</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Saravanan Baskaran.
23 Dec 2017
</p>
</div>
</div>

<div id="outline-container-org40d4522" class="outline-3">
<h3 id="org40d4522"><span class="section-number-3">1.2</span> Proposal</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org4f6144f" class="outline-4">
<h4 id="org4f6144f"><span class="section-number-4">1.2.1</span> Domain Background</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Now a days there are lot of data produced which are visual in nature. Many a times we get pictures and videos shared in social media in addition to text. In order to find the sentiment of a message it becomes necessary to use the visual content in addition to the textual content.This project attempts to build and train a deep learning model based on a research paper that classify an image as expressing positive, negative or neutral sentiment. The proposed model will be trained based on data collected from wild, ie without using any manually labeled data.
</p>

<p>
This project is based on the paper &#x2013; Cross-Media Learning for Image Sentiment Analysis in the Wild by
Lucia Vadicamo et al. Where they have outlined a method to use cross media learning to train a visual sentiment classifier. This is a first of kind approach where a classifier is trained using data from wild.
</p>

<p>
This is a very interesting pioneering effort in the field of visual sentiment classification. We try to replicate their results from the image classification part and then try to improve upon it. We will train models using the latest CNN architectures from imagenet contest <a href="http://www.image-net.org/challenges/LSVRC/">ILSVRC</a>, namely Inception and Mobilenet and compare their performance (accuracy, number of parameters and prediction speed) to the implementation of original models as described in the research paper. We will also evaluate our models with another human labeled dataset available at <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html">DeepSent</a> to check if we get comparable accuracy scores listed in the <a href="https://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf">paper</a> available from the site.
</p>
</div>
</div>

<div id="outline-container-org86f62b3" class="outline-4">
<h4 id="org86f62b3"><span class="section-number-4">1.2.2</span> Problem Statement</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
This is a multi-class classification problem where we build models to classify an image as expressing positive, negative or neutral sentiment. We train the model with the images and labels from B-T4SA dataset. Once trained the model will be able to take an image as input and classifies it into one of the three output labels positive, negative or neutral.
</p>

<p>
We will use the labeled data from the B-T4SA dataset to train our models. We try to replicate the results from the experiment using the architecture described in the paper.
It will be trained with the downloaded dataset and set as baseline models.
</p>

<p>
We will try to improve upon the result by fine tuning two models based on InceptionV3 and two on Mobilenet architectures trained on imagenet data. We will compare the new models performance (accuracy, prediction time and number of parameters) to the baseline model. All these models will be trained using the same training and validation data and tested against the test set from the downloaded B-T4SA dataset and TDD dataset found at <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html">DeepSent</a>.
</p>
</div>
</div>

<div id="outline-container-orgfb687ee" class="outline-4">
<h4 id="orgfb687ee"><span class="section-number-4">1.2.3</span> Datasets and Inputs</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
As part of the paper, Lucia Vadicamo et al collected a dataset containing 3 million tweets with text and images. The images are labeled based on the content of the text in the tweet using an accurate classifier which works on text. It is then preprocessed to create a labeled image dataset containing balanced number of images(156,862) for each of the three classes. The dataset is available online and can be downloaded from <a href="http://www.t4sa.it">T4SA</a>
</p>

<p>
We use the B-T4SA dataset to train and test our models. The data set consist of colored images collected from twitter which are labeled as positive, neutral or negative based on the text content by a classifier. The images are preprocessed to remove duplicates and near duplicates. To balance the distribution among the three classes the number of tweet images N(156, 862) from the negative class is selected as maximum for each class. N number of images are picked from the other two classes without replacement to get the final dataset.  We finally have 156,862 images for each category with a total of 470,586 images. This is a balanced dataset which contains equal number of samples from each category (positive, negative and neutral).This data is split approximately into 80% for training, 10% for validation and 10% for testing.
</p>
</div>
</div>

<div id="outline-container-orgb5ad733" class="outline-4">
<h4 id="orgb5ad733"><span class="section-number-4">1.2.4</span> Solution Statement</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
We will follow an approach as described in the T4SA paper to construct our solution. We will use a CNN model to classify the images. Training a deep CNN model from scratch is hard since we don't have sufficient data to learn all filters. To solve this we use a pre trained model that is trained on imagenet data as our feature extractor and then fine tune on it using our data. We use two types of fine tuning. In one instance we lock all layers except FC and only fine tune the FC layers. In another instance we fine tune all layers with the new data.
</p>

<p>
We will use a Vgg19 model trained on imagenet and finetune all its layers and call it Vgg19-T4SA-A and only the FC layers and call it Vgg19-T4SA-F. We should check that we achieve an accuracy as listed in the table on B-T4SA test set and on <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.htm">DeepSent</a> test data which is called Twitter Testing Data, abbreviated as TTD. TTD had 3 sets of data based on the number of workers agreement to the classification. We have TTD 5 where all the 5 workers agreed on a label and it contains data with high confidence. TTD 4 contains samples where 4 workers agreed on the majority label and similarly TTD 3 contains samples where 3 agreed on the majority label. These models and their accuracy scores are used as baseline to test and compare to other models.
</p>

<p>
[Tabel1]
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-right">TTD 5 agree</th>
<th scope="col" class="org-right">TTD 4 agree</th>
<th scope="col" class="org-right">TTD 3 agree</th>
<th scope="col" class="org-right">B-T4SA Testset</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Vgg19-T4SA-F</td>
<td class="org-right">0.768</td>
<td class="org-right">0.737</td>
<td class="org-right">0.715</td>
<td class="org-right">0.506</td>
</tr>

<tr>
<td class="org-left">Vgg19-T4SA-A</td>
<td class="org-right">0.785</td>
<td class="org-right">0.755</td>
<td class="org-right">0.725</td>
<td class="org-right">0.513</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
We finetune two models based on InceptionV3 architecture trained on imagenet with the B-T4SA training dataset. We finetune all layers in model and call in InceptionV3-T4SA-A and only the FC layers in another model and call it InceptionV3-T4SA-F. We finetune two more models based on Mobilenet architecture on all and FC layers and call them Mobilenet-T4SA-A and Mobilenet-T4SA-F respectively
</p>

<p>
Both model must achieve comparable performance to the Vgg19-T4SA-F and Vgg19-T4SA-A. The accuracy on TTD and B-T4SA should be within 5% of the accuracy of the baseline models. We expect the new models to have a better accuracy and performance than the baseline models.
</p>
</div>
</div>

<div id="outline-container-org7812995" class="outline-4">
<h4 id="org7812995"><span class="section-number-4">1.2.5</span> Benchmark Model</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
We use the  Vgg19-T4SA-F and Vgg19-T4SA-A models that we constructed based on the methods outlined in T4SA <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w5/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.pdf">paper</a> as benchmark models for score and performance metrics.
</p>

<p>
We compare the score and performance of our models namely InceptionV3-T4SA-F, InceptionV3-T4SA-A, Mobilenet-T4SA-F, Mobilenet-T4SA-A to that of the baseline models.
</p>

<p>
We expect our model to have comparable or better score and performance to the base models.
</p>
</div>
</div>

<div id="outline-container-orgc3a2384" class="outline-4">
<h4 id="orgc3a2384"><span class="section-number-4">1.2.6</span> Evaluation Metrics</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
The models are evaluated on the accuracy score they reach on the TTD 5, TTD 4, TTD 3 and B-T4SA test set.
We record the prediction time on TTD5 test set to find the model that takes less time in predicting the result. We also compare the number of parameters of each model which acts as surrogate measure for memory usage of the models.
</p>

<p>
We expect the new models InceptionV3-T4SA-F, InceptionV3-T4SA-A, Mobilenet-T4SA-F, Mobilenet-T4SA-A to be better than baseline models in terms of accuracy and performance metrics we defined.
</p>
</div>
</div>

<div id="outline-container-org28524b2" class="outline-4">
<h4 id="org28524b2"><span class="section-number-4">1.2.7</span> Project Design</h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
We use keras running on top of tensorflow to design our models. Keras provides templates for VGG19, InceptionV3 and Mobilenet along with the weights trained on imagenet which we will use as a base to train our models. We will reshape the images from B-T4SA and TDD dataset to 228x228 to match the imagenet image size. Keras also take care of centering the image pixels in all three channel to zero by subtracting the mean pixel values of each of the RGB channels from the input image.
</p>

<p>
We will use the VGG19 trained on imagenet and finetune all the layers and only fully connected layers on two instances and get Vgg19-T4SA-A and Vgg19-T4SA-F which are set as baseline models. The results from this model will be compared to the results in <a href="http://www.t4sa.it/">T4SA</a> to check if our baseline is correct. They should reach an accuracy as given in table Table1 on the TTD and B-T4SA dataset.
</p>

<p>
Next we finetune a model based on InceptionV3 architecture trained on imagenet. We finetune two instance of this model, where we finetune FC layers for one of the model and all layers in another. We get two models finetuned on two different set of layers InceptionV3-T4SA-F, InceptionV3-T4SA-A which will be compared with the base model. Similarly we will also finetune a set of models  Mobilenet-T4SA-F, Mobilenet-T4SA-A  based on Mobilenet architecture which will be used in the comparison against the base model.
</p>

<p>
We graph the results and present the accuracy, prediction time, number of parameters of InceptionV3-T4SA-F and Mobilenet-T4SA-F as a multiple of Vgg19-T4SA-F and InceptionV3-T4SA-A and Mobilenet-T4SA-A as a multiple of Vgg19-T4SA-A model in three different bar charts.
</p>

<p>
We tabulated the accuracy of Vgg19-T4SA-F, Vgg19-T4SA-A, InceptionV3-T4SA-F, InceptionV3-T4SA-A, Mobilenet-T4SA-F, Mobilenet-T4SA-A on TDD5, TDD4 and TDD3 along with the results from other models listed in the paper at <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html">DeepSent</a>
</p>

<p>
It is expected that the InceptionV3 based model to train faster and more accurate than the baseline model. The Mobilenet based model is expected to perform well in memory utilization and in prediction in terms of running time and may have less accuracy than baseline. We will try to tune the Mobilenet model to give as good accuracy as baseline models.
</p>
</div>
</div>

<div id="outline-container-orgdb32f65" class="outline-4">
<h4 id="orgdb32f65"><span class="section-number-4">1.2.8</span> Future Work</h4>
<div class="outline-text-4" id="text-1-2-8">
<ul class="org-ul">
<li>Improve the accuracy of classifier by using both the image and textual features together and check its effect on the TTD and B-T4SA test set.</li>
<li>Collect data using amazon mechanical truck on tweets containing both text and images in English and check the performance of the combined model on this human labeled data.</li>
<li>Create a web application using the top performing model that classifies an image given an url as having positive, negative or neutral sentiment content.</li>
<li>Do the reverse. Finetune an InceptionV3 model trained on imagenet with augmented version on images from DeepSenti and compare the performance on  B-T4SA test set to see if we can get away with less data</li>
<li>Explore on using the complete data set from T4SA to further improve the scores. Use class weights to account for unbalanced class distribution in the dataset.</li>
</ul>
</div>
</div>


<div id="outline-container-org4a2a28e" class="outline-4">
<h4 id="org4a2a28e"><span class="section-number-4">1.2.9</span> Reference</h4>
<div class="outline-text-4" id="text-1-2-9">
<ul class="org-ul">
<li>T4SA &#x2013; <a href="http://www.t4sa.it/">http://www.t4sa.it/</a></li>
<li>DeepSent &#x2013; <a href="https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html">https://www.cs.rochester.edu/u/qyou/DeepSent/deepsentiment.html</a></li>
<li>Cross-Media Learning for Image Sentiment Analysis in the Wild
Lucia Vadicamo, Fabio Carrara, Andrea Cimino, Stefano Cresci, Felice Dell'Orletta, Fabrizio Falchi, Maurizio Tesconi <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w5/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.pdf">link</a></li>
<li>Quanzeng You, Jiebo Luo, Hailin Jin and Jianchao Yang, "Robust Image Sentiment Analysis using Progressively Trained and Domain Transferred Deep Networks", the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI), Austin, TX, January 25-30, 2015.<a href="https://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf">link</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Saravanan Baskaran</p>
<p class="date">Created: 2017-12-24 Sun 04:49</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
